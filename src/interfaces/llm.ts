import type { Message } from '../types/common';

/**
 * Defines the essential operations for a Large Language Model (LLM).
 * This interface provides a standardized way to interact with various LLM implementations,
 * covering model lifecycle (loading, unloading) and core text generation capabilities.
 * It supports streaming of generated tokens for interactive applications.
 */
export interface LLMInterface {
  /**
   * Loads the LLM model resources (e.g., weights, tokenizer) into memory.
   * This should be called before attempting to generate text.
   * @returns A promise that resolves to the instance of the LLM once loaded.
   */
  load: () => Promise<this>;

  /**
   * Interrupts any ongoing text generation process.
   * This can be useful for stopping long-running generations prematurely.
   */
  interrupt: () => void;

  /**
   * Deletes or unloads the LLM model and its associated resources from memory.
   * This is typically used to free up system resources when the model is no longer needed.
   */
  delete: () => void;

  /**
   * Generates a text response based on a sequence of messages.
   * The callback function allows for streaming tokens as they are generated.
   * @param messages An array of message objects representing the conversation history.
   * @param callback A function that is called with each new token generated by the LLM.
   * @returns A promise that resolves to the complete generated string.
   */
  generate: (
    messages: Message[],
    callback: (token: string) => void
  ) => Promise<string>;
}

/**
 * Abstract base class for all Large Language Model (LLM) implementations.
 * This class implements the LLMInterface and provides a common structure
 * for handling LLM lifecycle and text generation. Concrete implementations
 * must provide the specific logic for loading, interacting with, and unloading
 * the underlying LLM.
 * @template LLMParams The type of parameters required by the specific LLM implementation.
 */
export declare abstract class LLM<LLMParams> implements LLMInterface {
  constructor(params: LLMParams);

  /**
   * Loads the LLM model resources (e.g., weights, tokenizer) into memory.
   * This should be called before attempting to generate text.
   * @returns A promise that resolves to the instance of the LLM once loaded.
   */
  abstract load: () => Promise<this>;

  /**
   * Interrupts any ongoing text generation process.
   * This can be useful for stopping long-running generations prematurely.
   */
  abstract interrupt: () => void;

  /**
   * Deletes or unloads the LLM model and its associated resources from memory.
   * This is typically used to free up system resources when the model is no longer needed.
   */
  abstract delete: () => void;

  /**
   * Generates a text response based on a sequence of messages.
   * The callback function allows for streaming tokens as they are generated.
   * @param messages An array of message objects representing the conversation history.
   * @param callback A function that is called with each new token generated by the LLM.
   * @returns A promise that resolves to the complete generated string.
   */
  abstract generate: (
    messages: Message[],
    callback: (token: string) => void
  ) => Promise<string>;
}
